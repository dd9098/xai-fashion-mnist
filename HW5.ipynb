{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Tuple, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from captum.attr import Lime, GradientShap, InputXGradient, IntegratedGradients\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import quantus\n",
    "from quantus import FaithfulnessCorrelation, RelativeInputStability, Sparseness\n",
    "sns.set()\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "quantus.AVAILABLE_XAI_METHODS_CAPTUM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preparation\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 0 # Increase for faster data loading\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_set = torchvision.datasets.FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=200, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Display a batch of images\n",
    "def display_images(images: torch.Tensor, labels: torch.Tensor, num_images: int = 7):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 3, 3))\n",
    "    for i in range(num_images):\n",
    "        axes[i].imshow(images[i].cpu().squeeze(), cmap=\"gray\")\n",
    "        axes[i].set_title(f\"Class: {labels[i].item()}\")\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "display_images(*next(iter(train_loader)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # First Convolutional Block\n",
    "        self.conv_1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # 28x28 -> 28x28\n",
    "        self.bn_1 = nn.BatchNorm2d(16)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 28x28 -> 14x14\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        self.conv_2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # 14x14 -> 14x14\n",
    "        self.bn_2 = nn.BatchNorm2d(32)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 14x14 -> 7x7\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc_1 = nn.Linear(32 * 7 * 7, 128)  # Adjusted from 256 → 128\n",
    "        self.relu_3 = nn.ReLU()\n",
    "        self.dropout_1 = nn.Dropout(0.4)  # Dropout for regularization\n",
    "        self.fc_2 = nn.Linear(128, 64)  # Adjusted from 120 → 64\n",
    "        self.relu_4 = nn.ReLU()\n",
    "        self.dropout_2 = nn.Dropout(0.4)  # Dropout for regularization\n",
    "        self.fc_3 = nn.Linear(64, 10)  # Output layer for 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional Layers\n",
    "        x = self.pool_1(self.relu_1(self.bn_1(self.conv_1(x))))\n",
    "        x = self.pool_2(self.relu_2(self.bn_2(self.conv_2(x))))\n",
    "\n",
    "        # Flatten for Fully Connected Layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu_3(self.dropout_1(self.fc_1(x)))\n",
    "        x = self.relu_4(self.dropout_2(self.fc_2(x)))\n",
    "        x = self.fc_3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "def train_model(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 0.005,\n",
    "    weight_decay: float = 1e-4\n",
    ") -> torch.nn.Module:\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "def evaluate_model(\n",
    "    model: torch.nn.Module,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    device: torch.device\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "    logits = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            logits.append(outputs)\n",
    "            targets.append(labels)\n",
    "    return torch.cat(logits), torch.cat(targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing\n",
    "model = LeNet()\n",
    "trained_model = train_model(model, train_loader, test_loader, device)\n",
    "\n",
    "# Evaluation\n",
    "predictions, labels = evaluate_model(trained_model, test_loader, device)\n",
    "test_accuracy = (predictions.argmax(dim=1) == labels).float().mean().item()\n",
    "print(f\"Final Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation methods\n",
    "explanations = {\n",
    "    \"Lime\": Lime(trained_model),\n",
    "    \"GradientShap\": GradientShap(trained_model),\n",
    "    \"InputXGradient\": InputXGradient(trained_model),\n",
    "    \"IntegratedGradients\": IntegratedGradients(trained_model),\n",
    "}\n",
    "\n",
    "x_batch, y_batch = next(iter(test_loader))\n",
    "x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized attributions\n",
    "attributions = {}\n",
    "for name, method in explanations.items():\n",
    "    if name == \"GradientShap\" or name == \"IntegratedGradients\":\n",
    "        baseline = torch.zeros_like(x_batch)\n",
    "        attributions[name] = quantus.normalise_by_negative(\n",
    "            method.attribute(inputs=x_batch, baselines=baseline, target=y_batch).sum(dim=1).cpu().detach().numpy()\n",
    "        )\n",
    "    else:\n",
    "        attributions[name] = quantus.normalise_by_negative(\n",
    "            method.attribute(inputs=x_batch, target=y_batch).sum(dim=1).cpu().detach().numpy()\n",
    "        )\n",
    "\n",
    "# Visualization\n",
    "for name, attr in attributions.items():\n",
    "    print(f\"{name} Attributions: {attr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_batch_lime = quantus.normalise_func.normalise_by_negative(Lime(model).attribute(inputs=x_batch, target=y_batch).sum(axis=1).detach().to(device).numpy())\n",
    "a_batch_gradient = quantus.normalise_func.normalise_by_negative(GradientShap(model).attribute(inputs=x_batch, baselines=torch.zeros_like(x_batch), target=y_batch).sum(axis=1).detach().to(device).numpy())\n",
    "a_batch_inputXgradient = quantus.normalise_func.normalise_by_negative(InputXGradient(model).attribute(inputs=x_batch, target=y_batch).sum(axis=1).detach().to(device).numpy())\n",
    "a_batch_intgrad = quantus.normalise_func.normalise_by_negative(IntegratedGradients(model).attribute(inputs=x_batch, target=y_batch, baselines=torch.zeros_like(x_batch)).sum(axis=1).detach().to(device).numpy())\n",
    "\n",
    "\n",
    "x_batch, y_batch = x_batch.to(device).numpy(), y_batch.to(device).numpy()\n",
    "\n",
    "assert [isinstance(obj, np.ndarray) for obj in [x_batch, y_batch, a_batch_lime, a_batch_gradient, a_batch_inputXgradient, a_batch_intgrad]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of attribution methods for dynamic processing\n",
    "attribution_methods = list(attributions.keys())\n",
    "\n",
    "# Plot attributions\n",
    "nr_images = 4\n",
    "fig, axes = plt.subplots(nrows=nr_images, ncols=len(attribution_methods) + 1, figsize=(15, 12))\n",
    "\n",
    "for i in range(nr_images):\n",
    "    # Input image\n",
    "    axes[i, 0].imshow((np.reshape(x_batch[i], (28, 28)) * 255).astype(np.uint8), cmap=\"gray\")\n",
    "    axes[i, 0].title.set_text(f\"FMNIST class {y_batch[i].item()}\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "    # Loop through attribution methods\n",
    "    for j, method in enumerate(attribution_methods):\n",
    "        axes[i, j + 1].imshow(attributions[method][i], cmap=\"seismic\", vmin=-0.95, vmax=1)\n",
    "        axes[i, j + 1].title.set_text(method)\n",
    "        axes[i, j + 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness Correlation\n",
    "Measures how well the explanation aligns with the model's predictions.\n",
    "https://quantus.readthedocs.io/en/latest/docs_api/quantus.metrics.faithfulness.faithfulness_correlation.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the FaithfulnessCorrelation metric\n",
    "faithfulness_metric = quantus.FaithfulnessCorrelation(\n",
    "    nr_runs=50, # Number of runs to compute the faithfulness score (default 200 was really slow)\n",
    "    subset_size=224,\n",
    "    perturb_baseline=\"black\",\n",
    "    perturb_func=quantus.perturb_func.baseline_replacement_by_indices,\n",
    "    similarity_func=quantus.similarity_func.correlation_pearson,\n",
    "    disable_warnings=True,\n",
    "    normalise=True,\n",
    "    abs=True\n",
    ")\n",
    "\n",
    "# Dictionary to store faithfulness scores for each method\n",
    "faithfulness_scores = {}\n",
    "\n",
    "# Compute faithfulness scores for each attribution method\n",
    "for method_name, method_attributions in attributions.items():\n",
    "    # Ensure attributions are torch tensors\n",
    "    method_attributions = torch.tensor(method_attributions, dtype=torch.float32, device=device)\n",
    "    \n",
    "    faithfulness_scores[method_name] = faithfulness_metric(\n",
    "        model=model,\n",
    "        x_batch=x_batch,\n",
    "        y_batch=y_batch,\n",
    "        a_batch=method_attributions,\n",
    "        device=device,\n",
    "        explain_func=quantus.explain,\n",
    "        explain_func_kwargs={\"method\": method_name}\n",
    "    )\n",
    "\n",
    "# Display the faithfulness scores\n",
    "faithfulness_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this evaluation, higher values suggest a stronger correspondence between the explanation and the actual model behavior.\n",
    "\n",
    "The results obtained are as follows:\n",
    "\n",
    "- Lime: 0.0071\n",
    "- GradientShap: 0.1066\n",
    "- InputXGradient: 0.1190\n",
    "- IntegratedGradients: 0.1195\n",
    "\n",
    "These findings suggest that IntegratedGradients and InputXGradient provide explanations more closely aligned with the model’s predictions, while the explanations generated by Lime and GradientShap appear to be less faithful in capturing the model’s underlying decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Input Stability\n",
    "Evaluates the robustness of explanations under slight input perturbations. The lower, the better.\n",
    "https://quantus.readthedocs.io/en/latest/docs_api/quantus.metrics.robustness.html  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RelativeInputStability metric\n",
    "relative_input_stability_metric = quantus.RelativeInputStability(\n",
    "    nr_samples=5, # Number of samples to compute the stability score (default 200 was really slow)\n",
    "    perturb_func=quantus.perturb_func.uniform_noise,\n",
    "    normalise=True,\n",
    "    disable_warnings=True,\n",
    "    abs=True\n",
    ")\n",
    "\n",
    "# Ensure tensors are on the correct device and dtype\n",
    "predictions, labels = evaluate_model(model, test_loader, device)\n",
    "predicted_labels = predictions.argmax(dim=1)\n",
    "\n",
    "# Filter correctly classified samples\n",
    "correct_indices = (predicted_labels == labels).nonzero(as_tuple=True)[0]\n",
    "print(f\"Number of correctly classified: {len(correct_indices)}\")\n",
    "\n",
    "correct_indices = correct_indices[correct_indices < len(x_batch)]\n",
    "\n",
    "# Ensure correct indices are within bounds\n",
    "correct_x_batch = x_batch[correct_indices]\n",
    "correct_y_batch = y_batch[correct_indices]\n",
    "\n",
    "# Dictionary to store stability scores\n",
    "relative_input_stability_scores = {}\n",
    "\n",
    "# Compute stability scores for each attribution method\n",
    "for method_name, method_attributions in attributions.items():\n",
    "    relative_input_stability_scores[method_name] = relative_input_stability_metric(\n",
    "        model=model,\n",
    "        x_batch=correct_x_batch,\n",
    "        y_batch=correct_y_batch,\n",
    "        a_batch=method_attributions[correct_indices],\n",
    "        device=device,\n",
    "        explain_func=quantus.explain,\n",
    "        explain_func_kwargs={\"method\": method_name}\n",
    "    )\n",
    "\n",
    "# Calculate average scores\n",
    "average_results = {\n",
    "    method: np.nanmean(scores) for method, scores in relative_input_stability_scores.items()\n",
    "}\n",
    "\n",
    "# Display the results\n",
    "average_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative Input Stability metric reflects how resistant each attribution method is to slight perturbations in the input data. In this case, lower values indicate more robust explanations.\n",
    "\n",
    "The average scores obtained are as follows:\n",
    "- Lime: 14.0141\n",
    "- GradientShap: 1.8824\n",
    "- InputXGradient: 1.9936\n",
    "- IntegratedGradients: 1.8695\n",
    "\n",
    "These results suggest that Lime’s explanations are significantly less stable compared to the other methods. In contrast, IntegratedGradients, GradientShap, and InputXGradient produce explanations that remain more consistent and robust when inputs are slightly altered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity\n",
    "Assesses the simplicity of explanations by checking how many features contribute significantly. \n",
    "https://quantus.readthedocs.io/en/latest/docs_api/quantus.metrics.complexity.sparseness.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Sparsity metric\n",
    "sparsity_metric = quantus.Sparseness(normalise=True)\n",
    "\n",
    "# Ensure tensors are on the correct device and dtype\n",
    "predictions, labels = evaluate_model(model, test_loader, device)\n",
    "predicted_labels = predictions.argmax(dim=1)\n",
    "\n",
    "# Filter correctly classified samples\n",
    "correct_indices = (predicted_labels == labels).nonzero(as_tuple=True)[0]\n",
    "print(f\"Number of correctly classified: {len(correct_indices)}\")\n",
    "\n",
    "correct_indices = correct_indices[correct_indices < len(x_batch)]\n",
    "\n",
    "# Extract correctly classified samples\n",
    "correct_x_batch = x_batch[correct_indices]\n",
    "correct_y_batch = y_batch[correct_indices]\n",
    "\n",
    "# Dictionary to store sparsity scores\n",
    "sparsity_scores = {}\n",
    "\n",
    "# Compute sparsity scores for each attribution method\n",
    "for method_name, method_attributions in attributions.items():\n",
    "    sparsity_scores[method_name] = sparsity_metric(\n",
    "        model=model,\n",
    "        x_batch=correct_x_batch,\n",
    "        y_batch=correct_y_batch,\n",
    "        a_batch=method_attributions[correct_indices],\n",
    "        device=device,\n",
    "        explain_func=quantus.explain,\n",
    "        explain_func_kwargs={\"method\": method_name}\n",
    "    )\n",
    "\n",
    "# Calculate average sparsity results\n",
    "average_sparsity_results = {\n",
    "    method: np.nanmean(scores) for method, scores in sparsity_scores.items()\n",
    "}\n",
    "\n",
    "# Display the results\n",
    "average_sparsity_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparsity indicate how simple or concentrated the explanations are by measuring how many features have a significant impact. Higher values suggest that only a few features dominate the explanation.\n",
    "\n",
    "The average sparsity scores are as follows:\n",
    "\n",
    "- Lime: 0.9835\n",
    "- GradientShap: 0.7650\n",
    "- InputXGradient: 0.7574\n",
    "- IntegratedGradients: 0.7642\n",
    "\n",
    "These results suggest that Lime’s explanations are more feature-sparse (relying on fewer dominant features), while GradientShap, IntegratedGradients, and InputXGradient yield explanations that are comparatively more distributed across multiple features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is any method consistently the best across all metrics, or do trade-offs exist?\n",
    "No single attribution method clearly outperforms the others across all three metrics. Instead, there are notable trade-offs:\n",
    "\n",
    "- Faithfulness Correlation: IntegratedGradients and InputXGradient yield higher scores, indicating their explanations more closely align with the model’s predictions. Lime and GradientShap underperform in this regard.\n",
    "\n",
    "- Relative Input Stability: IntegratedGradients, GradientShap, and InputXGradient produce more stable explanations. Lime, despite its high sparsity, is notably less stable.\n",
    "\n",
    "- Sparsity: Lime’s explanations are the most feature-sparse, focusing on fewer dominant features. However, this simplicity comes at the cost of reduced faithfulness and stability.\n",
    "\n",
    "In conclusion, IntegratedGradients and InputXGradient strike a strong balance between faithfulness and stability, while Lime provides highly sparse explanations but lags in other areas. This indicates that the “best” method depends on the particular requirements and priorities for explanation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does the quantitative evaluation align with the qualitative observations?\n",
    "The quantitative results are largely consistent with the qualitative observations. Lime explanations, which the metrics indicate are highly sparse but less faithful and stable, visually appear to highlight only a handful of pixels, often failing to capture the overall shape of the object. On the other hand, methods like IntegratedGradients and InputXGradient, which scored higher in both faithfulness and stability, produce attributions that more closely follow the contours and features of the object. GradientShap’s results, though less sparse, provide more evenly distributed attributions that align better with the underlying structure than Lime does.\n",
    "\n",
    "In essence, the metrics’ outcomes and the visual inspection both point towards the conclusion that methods yielding more consistent, broadly distributed attributions (e.g., IntegratedGradients and InputXGradient) produce explanations that qualitatively appear more coherent and faithful to the model’s reasoning, while Lime’s sparse yet unstable explanations resonate with its poorer performance on the quantitative evaluations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai-fashion-mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
